# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oxV78mYVH5FcbRv_T82QE1zF8BMla7Rq
"""

# Upload the file "gridworld_maze.py"

import numpy as np
from gridworld_maze import GridWorldMazeEnv
env = GridWorldMazeEnv(seed=0)

# Import packages. Run this cell.

import numpy as np
import gym
import torch
import torch.nn as nn
import torch.nn.functional as F
from collections import deque
import random

"""
A simple Q-network class
"""
class QNetwork(torch.nn.Module):
    def __init__(self, input_dim, output_dim, hidden_dim):
        """
        Args:
            input_dim (int): state dimension.
            output_dim (int): number of actions.
            hidden_dim (int): hidden layer dimension (fully connected layer)
        """
        super().__init__()
        self.linear1 = nn.Linear(input_dim, hidden_dim)
        self.linear2 = nn.Linear(hidden_dim, output_dim)

    def forward(self, state):
        """
        Returns a Q value
        Args:
            state (torch.Tensor): state, 2-D tensor of shape (n, input_dim)
        Returns:
            torch.Tensor: Q values, 2-D tensor of shape (n, output_dim)
        """
        x = F.relu(self.linear1(state))
        x = self.linear2(x)
        return x


"""
Agent class that implements the DQN algorithm
"""
class DQN:
    def __init__(self, seed=None):
        self.output_dim = 3  # Output dimension of Q network, i.e., the number of possible actions
        self.dqn = QNetwork(4, self.output_dim, 128)  # Q network
        self.dqn_target = QNetwork(4, self.output_dim, 128)  # Target Q network
        self.dqn_target.load_state_dict(self.dqn.state_dict())
        self.batch_size = 64  # Batch size
        self.gamma = 0.99  # Discount factor
        self.eps = 1.0  # epsilon-greedy for exploration
        self.loss_fn = torch.nn.MSELoss()  # loss function
        self.optim = torch.optim.Adam(self.dqn.parameters(), lr=1e-3)  # optimizer for training
        self.replay_memory_buffer = deque(maxlen=10000)  # replay buffer
        if seed is None:
            self.rng = np.random.default_rng()
        else:
            self.rng = np.random.default_rng(seed)
    
    def select_action(self, state):
        """
        Returns an action for the agent to take during training process
        Args:
            state: a numpy array with size 4
        Returns:
            action: an integer, 0 or 1
        """
        
        # Please complete codes for choosing an action given the current state
        """
        Hint: You may use epsilon-greedy for exploration. 
        With probability self.eps, choose an action uniformly at random; 
        Otherwise, choose a greedy action based on the output of the Q network (self.dqn).
        """
        ### BEGIN SOLUTION
        # YOUR CODE HERE
        prob = self.rng.random()
        if prob < self.eps:
            action = np.random.randint(3)
        elif prob >= self.eps:
            state = torch.from_numpy(state).float()
            with torch.no_grad():
              action = torch.argmax(self.dqn(state)).item()

        ### END SOLUTION
        return action

    def train(self, s0, a0, r, s1, done):
        """
        Train the Q network
        Args:
            s0: current state, a numpy array with size 4
            a0: current action, 0 or 1
            r: reward
            s1: next state, a numpy array with size 4
            done: done=True means that the episode terminates and done=False means that the episode does not terminate.
        """
        self.add_to_replay_memory(s0, a0, r, s1, done)
        
        if done:
            self.update_epsilon()
            self.target_update()
            
        if len(self.replay_memory_buffer) < self.batch_size:
            return
        
        """
        state_batch: torch.Tensor with shape (self.batch_size, 4), a mini-batch of current states
        action_batch: torch.Tensor with shape (self.batch_size, 1), a mini-batch of current actions
        reward_batch: torch.Tensor with shape (self.batch_size, 1), a mini-batch of rewards
        next_state_batch: torch.Tensor with shape (self.batch_size, 4), a mini-batch of next states
        done_list: torch.Tensor with shape (self.batch_size, 1), a mini-batch of 0-1 integers, 
                   where 1 means the episode terminates for that sample;
                         0 means the episode does not terminate for that sample.
        """
        mini_batch = self.get_random_sample_from_replay_mem()
        state_batch = torch.from_numpy(np.vstack([i[0] for i in mini_batch])).float()
        action_batch = torch.from_numpy(np.vstack([i[1] for i in mini_batch])).int()
        reward_batch = torch.from_numpy(np.vstack([i[2] for i in mini_batch])).float()
        next_state_batch = torch.from_numpy(np.vstack([i[3] for i in mini_batch])).float()
        done_list = torch.from_numpy(np.vstack([i[4] for i in mini_batch]).astype(np.uint8)).float()
        # Please complete codes for updating the Q network self.dqn
        """
        Hint: You may use the above tensors: state_batch, action_batch, reward_batch, next_state_batch, done_list
              You may use self.dqn_target as your target Q network
              You may use self.loss_fn (or torch.nn.MSELoss()) as your loss function
              You may use self.optim as your optimizer for training the Q network
        """
        ### BEGIN SOLUTION
        # YOUR CODE HERE    
        state_action_values = self.dqn(state_batch).gather(1, action_batch.long())
        next_state_values = self.dqn_target(next_state_batch).max(1)[0].view(-1,1)
        target_values = reward_batch + torch.mul(self.gamma * next_state_values, 1 - done_list).detach()
       
        loss = self.loss_fn(state_action_values, target_values)
        self.optim.zero_grad()
        loss.backward()
        self.optim.step()
        ### END SOLUTION
        return

    def add_to_replay_memory(self, state, action, reward, next_state, done):
        """
        Add samples to replay memory
        Args:
            state: current state, a numpy array with size 4
            action: current action, 0 or 1
            reward: reward
            next_state: next state, a numpy array with size 4
            done: done=True means that the episode terminates and done=False means that the episode does not terminate.
        """
        self.replay_memory_buffer.append((state, action, reward, next_state, done))

    def get_random_sample_from_replay_mem(self):
        """
        Random samples from replay memory without replacement
        Returns a self.batch_size length list of unique elements chosen from the replay buffer.
        Returns:
            random_sample: a list with len=self.batch_size,
                           where each element is a tuple (state, action, reward, next_state, done)
        """
        random_sample = random.sample(self.replay_memory_buffer, self.batch_size)
        return random_sample
    
    def update_epsilon(self):
        # Decay epsilon
        if self.eps >= 0.01:
            self.eps *= 0.95
    
    def target_update(self):
        # Update the target Q network (self.dqn_target) using the original Q network (self.dqn)
        self.dqn_target.load_state_dict(self.dqn.state_dict())

